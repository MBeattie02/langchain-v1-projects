{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-1.0.8-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-chroma\n",
      "  Downloading langchain_chroma-1.0.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langchain-ollama\n",
      "  Using cached langchain_ollama-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-6.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.0.6 (from langchain)\n",
      "  Downloading langchain_core-1.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n",
      "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
      "  Using cached langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (13 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Downloading langsmith-0.4.44-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/matthewbeattie/Library/Python/3.13/lib/python/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (25.0)\n",
      "Collecting pyyaml<7.0.0,>=5.3.0 (from langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Downloading pyyaml-6.0.3-cp313-cp313-macosx_10_13_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.7.0 (from langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph)\n",
      "  Downloading ormsgpack-1.12.0-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (1.2 kB)\n",
      "Collecting httpx>=0.25.2 (from langgraph-sdk<0.3.0,>=0.2.2->langgraph)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.3.0,>=0.2.2->langgraph)\n",
      "  Downloading orjson-3.11.4-cp313-cp313-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting requests>=2.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Downloading zstandard-0.25.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting anyio (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting certifi (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph)\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.41.5-cp313-cp313-macosx_10_12_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting chromadb<2.0.0,>=1.0.20 (from langchain-chroma)\n",
      "  Downloading chromadb-1.3.5-cp39-abi3-macosx_10_12_x86_64.whl.metadata (7.2 kB)\n",
      "Collecting numpy>=2.1.0 (from langchain-chroma)\n",
      "  Downloading numpy-2.3.5-cp313-cp313-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Collecting build>=1.0.3 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pybase64>=1.4.1 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading pybase64-1.4.2-cp313-cp313-macosx_10_13_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading onnxruntime-1.23.2-cp313-cp313-macosx_13_0_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Using cached opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Using cached opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Using cached PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tqdm>=4.65.0 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting overrides>=7.3.1 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading grpcio-1.76.0-cp313-cp313-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-macosx_10_12_universal2.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading mmh3-5.2.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (14 kB)\n",
      "Collecting rich>=10.11.0 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting jsonschema>=4.19.0 (from chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/matthewbeattie/Library/Python/3.13/lib/python/site-packages (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /Users/matthewbeattie/Library/Python/3.13/lib/python/site-packages (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (2.9.0.post0)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting distro>=1.5.0 (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Downloading charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl.metadata (37 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting ollama<1.0.0,>=0.6.0 (from langchain-ollama)\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
      "  Using cached langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain-community)\n",
      "  Downloading sqlalchemy-2.0.44-cp313-cp313-macosx_10_13_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.13.2-cp313-cp313-macosx_10_13_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.8.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.7.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.4.1-cp313-cp313-macosx_10_13_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.22.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (75 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
      "  Using cached langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community)\n",
      "  Downloading greenlet-3.2.4-cp313-cp313-macosx_11_0_universal2.whl.metadata (4.1 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading rpds_py-0.29.0-cp313-cp313-macosx_10_12_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting cachetools<7.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Using cached opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Using cached opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/matthewbeattie/Library/Python/3.13/lib/python/site-packages (from rich>=10.11.0->chromadb<2.0.0,>=1.0.20->langchain-chroma) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading huggingface_hub-1.1.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading httptools-0.7.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (3.5 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading uvloop-0.22.1-cp313-cp313-macosx_10_13_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading watchfiles-1.1.1-cp313-cp313-macosx_10_12_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading websockets-15.0.1-cp313-cp313-macosx_10_13_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib->kubernetes>=28.1.0->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb<2.0.0,>=1.0.20->langchain-chroma)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading langchain-1.0.8-py3-none-any.whl (93 kB)\n",
      "Downloading langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
      "Downloading langchain_core-1.0.7-py3-none-any.whl (472 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
      "Using cached langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "Downloading langsmith-0.4.44-py3-none-any.whl (410 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp313-cp313-macosx_10_12_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyyaml-6.0.3-cp313-cp313-macosx_10_13_x86_64.whl (181 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading langchain_chroma-1.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading chromadb-1.3.5-cp39-abi3-macosx_10_12_x86_64.whl (20.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl (208 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached langchain_ollama-1.0.0-py3-none-any.whl (29 kB)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp313-cp313-macosx_10_13_x86_64.whl (490 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Using cached langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.7.0-cp313-cp313-macosx_10_13_x86_64.whl (45 kB)\n",
      "Downloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "Downloading sqlalchemy-2.0.44-cp313-cp313-macosx_10_13_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.22.0-cp313-cp313-macosx_10_13_x86_64.whl (93 kB)\n",
      "Downloading pypdf-6.3.0-py3-none-any.whl (328 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading bcrypt-5.0.0-cp39-abi3-macosx_10_12_universal2.whl (495 kB)\n",
      "Downloading build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading frozenlist-1.8.0-cp313-cp313-macosx_10_13_x86_64.whl (49 kB)\n",
      "Downloading greenlet-3.2.4-cp313-cp313-macosx_11_0_universal2.whl (272 kB)\n",
      "Downloading grpcio-1.76.0-cp313-cp313-macosx_11_0_universal2.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading mmh3-5.2.0-cp313-cp313-macosx_10_13_x86_64.whl (40 kB)\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading numpy-2.3.5-cp313-cp313-macosx_14_0_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.23.2-cp313-cp313-macosx_13_0_x86_64.whl (19.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
      "Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Using cached opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "Downloading protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Downloading orjson-3.11.4-cp313-cp313-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (243 kB)\n",
      "Downloading ormsgpack-1.12.0-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (369 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading propcache-0.4.1-cp313-cp313-macosx_10_13_x86_64.whl (44 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading pybase64-1.4.2-cp313-cp313-macosx_10_13_x86_64.whl (38 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading rpds_py-0.29.0-cp313-cp313-macosx_10_12_x86_64.whl (375 kB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-1.1.5-py3-none-any.whl (516 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Downloading httptools-0.7.1-cp313-cp313-macosx_10_13_universal2.whl (202 kB)\n",
      "Downloading uvloop-0.22.1-cp313-cp313-macosx_10_13_x86_64.whl (751 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.8/751.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.1-cp313-cp313-macosx_10_12_x86_64.whl (404 kB)\n",
      "Using cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
      "Downloading websockets-15.0.1-cp313-cp313-macosx_10_13_x86_64.whl (173 kB)\n",
      "Downloading xxhash-3.6.0-cp313-cp313-macosx_10_13_x86_64.whl (32 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading zstandard-0.25.0-cp313-cp313-macosx_10_13_x86_64.whl (795 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m795.7/795.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=3e5e24aa206177719b23df3b1718fda2391e6619545acf782777765986d2070e\n",
      "  Stored in directory: /Users/matthewbeattie/Library/Caches/pip/wheels/b4/f8/a5/28e9c1524d320f4b8eefdce0e487b5c2e128dbf2ed1bb4a60b\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, mpmath, flatbuffers, durationpy, zstandard, zipp, xxhash, websockets, websocket-client, uvloop, urllib3, typing-extensions, tqdm, tenacity, sympy, sniffio, shellingham, rpds-py, pyyaml, python-dotenv, pyproject_hooks, pypdf, pybase64, pyasn1, protobuf, propcache, overrides, ormsgpack, orjson, oauthlib, numpy, mypy-extensions, multidict, mmh3, mdurl, marshmallow, jsonpointer, importlib-resources, idna, humanfriendly, httpx-sse, httptools, hf-xet, h11, greenlet, fsspec, frozenlist, filelock, distro, click, charset_normalizer, certifi, cachetools, bcrypt, backoff, attrs, annotated-types, aiohappyeyeballs, yarl, uvicorn, typing-inspection, typing-inspect, typer-slim, SQLAlchemy, rsa, requests, referencing, pydantic-core, pyasn1-modules, opentelemetry-proto, markdown-it-py, jsonpatch, importlib-metadata, httpcore, grpcio, googleapis-common-protos, coloredlogs, build, anyio, aiosignal, watchfiles, rich, requests-toolbelt, requests-oauthlib, pydantic, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, jsonschema-specifications, httpx, google-auth, dataclasses-json, aiohttp, typer, pydantic-settings, opentelemetry-semantic-conventions, ollama, langsmith, langgraph-sdk, kubernetes, jsonschema, huggingface-hub, tokenizers, opentelemetry-sdk, langchain-core, opentelemetry-exporter-otlp-proto-grpc, langgraph-checkpoint, langchain-text-splitters, langchain-ollama, langgraph-prebuilt, langchain-classic, chromadb, langgraph, langchain-community, langchain-chroma, langchain\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117/117\u001b[0m [langchain]angchain]angchain-community]]tions]\n",
      "\u001b[1A\u001b[2KSuccessfully installed SQLAlchemy-2.0.44 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.11.0 attrs-25.4.0 backoff-2.2.1 bcrypt-5.0.0 build-1.3.0 cachetools-6.2.2 certifi-2025.11.12 charset_normalizer-3.4.4 chromadb-1.3.5 click-8.3.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 distro-1.9.0 durationpy-0.10 filelock-3.20.0 flatbuffers-25.9.23 frozenlist-1.8.0 fsspec-2025.10.0 google-auth-2.43.0 googleapis-common-protos-1.72.0 greenlet-3.2.4 grpcio-1.76.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httptools-0.7.1 httpx-0.28.1 httpx-sse-0.4.3 huggingface-hub-1.1.5 humanfriendly-10.0 idna-3.11 importlib-metadata-8.7.0 importlib-resources-6.5.2 jsonpatch-1.33 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 kubernetes-34.1.0 langchain-1.0.8 langchain-chroma-1.0.0 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.0.7 langchain-ollama-1.0.0 langchain-text-splitters-1.0.0 langgraph-1.0.3 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.2.9 langsmith-0.4.44 markdown-it-py-4.0.0 marshmallow-3.26.1 mdurl-0.1.2 mmh3-5.2.0 mpmath-1.3.0 multidict-6.7.0 mypy-extensions-1.1.0 numpy-2.3.5 oauthlib-3.3.1 ollama-0.6.1 onnxruntime-1.23.2 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 orjson-3.11.4 ormsgpack-1.12.0 overrides-7.7.0 posthog-5.4.0 propcache-0.4.1 protobuf-6.33.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybase64-1.4.2 pydantic-2.12.4 pydantic-core-2.41.5 pydantic-settings-2.12.0 pypdf-6.3.0 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.2.1 pyyaml-6.0.3 referencing-0.37.0 requests-2.32.5 requests-oauthlib-2.0.0 requests-toolbelt-1.0.0 rich-14.2.0 rpds-py-0.29.0 rsa-4.9.1 shellingham-1.5.4 sniffio-1.3.1 sympy-1.14.0 tenacity-9.1.2 tokenizers-0.22.1 tqdm-4.67.1 typer-0.20.0 typer-slim-0.20.0 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.2 urllib3-2.3.0 uvicorn-0.38.0 uvloop-0.22.1 watchfiles-1.1.1 websocket-client-1.9.0 websockets-15.0.1 xxhash-3.6.0 yarl-1.22.0 zipp-3.23.0 zstandard-0.25.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain langgraph langchain-chroma langchain-ollama langchain-community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: DOCUMENTS AND DOCUMENT LOADERS\n",
    "# ============================================================================\n",
    "# Load PDF - works with both online URLs and local file paths\n",
    "pdf_url = \"/Users/matthewbeattie/Desktop/Books/Docker up and running/4. Working with Docker Images _ Docker_ Up & Running, 3rd Edition.pdf\"\n",
    "loader = PyPDFLoader(pdf_url)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Loaded 59 pages from PDF\n"
     ]
    }
   ],
   "source": [
    "documents[0]\n",
    "print(f\"\\n✓ Loaded {len(documents)} pages from PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Document Structure:\n",
      "- Content length: 2028 characters\n",
      "- Metadata: {'producer': 'Skia/PDF m114', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36', 'creationdate': '2023-06-22T20:52:42+00:00', 'moddate': '2023-06-22T20:52:42+00:00', 'source': '/Users/matthewbeattie/Desktop/Books/Docker up and running/4. Working with Docker Images _ Docker_ Up & Running, 3rd Edition.pdf', 'total_pages': 59, 'page': 0, 'page_label': '1'}\n",
      "- Content preview: Chapter 4. Working with Docker Images\n",
      "Every Linux container is based on an image. Images are the underlying definition of what\n",
      "gets reconstituted into a running container, much like a virtual disk bec...\n"
     ]
    }
   ],
   "source": [
    "sample_doc = documents[0]\n",
    "print(f\"\\nSample Document Structure:\")\n",
    "print(f\"- Content length: {len(sample_doc.page_content)} characters\")\n",
    "print(f\"- Metadata: {sample_doc.metadata}\")\n",
    "print(f\"- Content preview: {sample_doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.1 Configuring Text Splitter...\n",
      "- Chunk size: 1024 characters (as specified)\n",
      "- Overlap: 100 characters (10% overlap)\n",
      "- Method: Recursive character splitting\n",
      "\n",
      "2.2 Splitting documents into chunks...\n",
      "\n",
      "✓ Split 59 pages into 135 chunks\n",
      "\n",
      "Chunk Analysis:\n",
      "- Average chunk size: 802 characters\n",
      "- Largest chunk: 1024 characters\n",
      "- Smallest chunk: 102 characters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: TEXT SPLITTING  \n",
    "# ============================================================================\n",
    "print(\"\\n2.1 Configuring Text Splitter...\")\n",
    "print(\"- Chunk size: 1024 characters (as specified)\")\n",
    "print(\"- Overlap: 100 characters (10% overlap)\")\n",
    "print(\"- Method: Recursive character splitting\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=100,  # 10% of 1024\n",
    "    length_function=len,\n",
    "    add_start_index=True,  # Preserves character index as metadata\n",
    ")\n",
    "\n",
    "print(\"\\n2.2 Splitting documents into chunks...\")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"\\n✓ Split {len(documents)} pages into {len(chunks)} chunks\")\n",
    "\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "print(f\"\\nChunk Analysis:\")\n",
    "print(f\"- Average chunk size: {sum(chunk_sizes) / len(chunk_sizes):.0f} characters\")\n",
    "print(f\"- Largest chunk: {max(chunk_sizes)} characters\")\n",
    "print(f\"- Smallest chunk: {min(chunk_sizes)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: EMBEDDINGS\n",
    "# ======\n",
    "# \n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.embed_query(\"Hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4.1 Creating Chroma Vector Store...\n",
      "- Collection name: pdf_collection\n",
      "- Storage: Local persistent directory\n",
      "- Embedding function: nomic-embed-text via Ollama\n",
      "✓ Added 135 document chunks to vector store\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: VECTOR STORES\n",
    "# ============================================================================\n",
    "print(\"\\n4.1 Creating Chroma Vector Store...\")\n",
    "print(\"- Collection name: pdf_collection\")\n",
    "print(\"- Storage: Local persistent directory\")\n",
    "print(\"- Embedding function: nomic-embed-text via Ollama\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"pdf_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=chunks)\n",
    "\n",
    "print(f\"✓ Added {len(chunks)} document chunks to vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.1 Basic Similarity Search\n",
      "Finding information about docker\n",
      "\n",
      "Query: 'What is the Anatomy of a Dockerfile'\n",
      "Retrieved 5 most similar chunks:\n",
      "\n",
      "--- Result 1 ---\n",
      "Content: with the Dockerfile. This file describes all the steps that are required to create an image\n",
      "and is usually contained within the root directory of the source code repository for your\n",
      "application.\n",
      "1...\n",
      "Source: Page 0\n",
      "\n",
      "--- Result 2 ---\n",
      "Content: image management, Docker relies heavily on this storage backend, which communicates\n",
      "with the underlying Linux filesystem to build and manage the multiple layers that com-\n",
      "bine into a single usable image. The primary storage backends that are supported include\n",
      "the following:\n",
      "Overlay2\n",
      "B-Tree File Syst...\n",
      "Source: Page 0\n",
      "\n",
      "--- Result 3 ---\n",
      "Content: A typical Dockerfile might look something like the one shown here, which creates a con-\n",
      "tainer for a Node.js-based application:\n",
      "FROM node:18.13.0\n",
      "ARG email=\"anna@example.com\"\n",
      "LABEL \"maintainer\"=$email\n",
      "LABEL \"rating\"=\"Five Stars\" \"class\"=\"First Class\"\n",
      "USER root\n",
      "ENV AP /data/app\n",
      "ENV SCPATH /etc/superv...\n",
      "Source: Page 1\n",
      "\n",
      "--- Result 4 ---\n",
      "Content: Chapter 4. Working with Docker Images\n",
      "Every Linux container is based on an image. Images are the underlying definition of what\n",
      "gets reconstituted into a running container, much like a virtual disk becomes a VM when\n",
      "you start it up. Docker or Open Container Initiative (OCI) images provide the basis f...\n",
      "Source: Page 0\n",
      "\n",
      "--- Result 5 ---\n",
      "Content: …\n",
      "   ]\n",
      "}\n",
      "If you look through the output, you will see that there are blocks that identify the image\n",
      "that is required for every platform the image supports. This is accomplished via the indi-\n",
      "vidual digest entries that are then paired with a platform block. This manifest file is\n",
      "downloaded by the ser...\n",
      "Source: Page 57\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: QUERYING THE VECTOR STORE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n5.1 Basic Similarity Search\")\n",
    "print(\"Finding information about docker\")\n",
    "\n",
    "query = \"What is the Anatomy of a Dockerfile\"\n",
    "results = vector_store.similarity_search(query, k=5)\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(f\"Retrieved {len(results)} most similar chunks:\")\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n--- Result {i} ---\")\n",
    "    print(f\"Content: {doc.page_content[:300]}...\")\n",
    "    print(f\"Source: Page {doc.metadata.get('page', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.2 Similarity Search with Scores\n",
      "Same search but with similarity scores to see confidence levels...\n",
      "\n",
      "--- Result 1 (Similarity Score: 0.4560) ---\n",
      "Content: with the Dockerfile. This file describes all the steps that are required to create an image\n",
      "and is usually contained within the root directory of the source code repository for your\n",
      "application.\n",
      "1...\n",
      "Source: Page 0\n",
      "\n",
      "--- Result 2 (Similarity Score: 0.5663) ---\n",
      "Content: image management, Docker relies heavily on this storage backend, which communicates\n",
      "with the underlying Linux filesystem to build and manage the multiple layers that com-\n",
      "bine into a single usable ima...\n",
      "Source: Page 0\n",
      "\n",
      "--- Result 3 (Similarity Score: 0.5907) ---\n",
      "Content: A typical Dockerfile might look something like the one shown here, which creates a con-\n",
      "tainer for a Node.js-based application:\n",
      "FROM node:18.13.0\n",
      "ARG email=\"anna@example.com\"\n",
      "LABEL \"maintainer\"=$email...\n",
      "Source: Page 1\n",
      "\n",
      "--- Result 4 (Similarity Score: 0.6160) ---\n",
      "Content: Chapter 4. Working with Docker Images\n",
      "Every Linux container is based on an image. Images are the underlying definition of what\n",
      "gets reconstituted into a running container, much like a virtual disk bec...\n",
      "Source: Page 0\n",
      "\n",
      "--- Result 5 (Similarity Score: 0.6434) ---\n",
      "Content: …\n",
      "   ]\n",
      "}\n",
      "If you look through the output, you will see that there are blocks that identify the image\n",
      "that is required for every platform the image supports. This is accomplished via the indi-\n",
      "vidual di...\n",
      "Source: Page 57\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5.2 Similarity Search with Scores\")\n",
    "print(\"Same search but with similarity scores to see confidence levels...\")\n",
    "\n",
    "results_with_scores = vector_store.similarity_search_with_score(query, k=5)\n",
    "\n",
    "for i, (doc, score) in enumerate(results_with_scores, 1):\n",
    "    print(f\"\\n--- Result {i} (Similarity Score: {score:.4f}) ---\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Source: Page {doc.metadata.get('page', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.3 Metadata Filtering\n",
      "Using metadata filters to search specific parts of the document...\n",
      "\n",
      "Available metadata in our chunks:\n",
      "Sample metadata: {'producer': 'Skia/PDF m114', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36', 'creationdate': '2023-06-22T20:52:42+00:00', 'moddate': '2023-06-22T20:52:42+00:00', 'source': '/Users/matthewbeattie/Desktop/Books/Docker up and running/4. Working with Docker Images _ Docker_ Up & Running, 3rd Edition.pdf', 'total_pages': 59, 'page': 0, 'page_label': '1', 'start_index': 0}\n",
      "Available page numbers (sample): [0, 1, 2, 3, 4]...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5.3 Metadata Filtering\")\n",
    "print(\"Using metadata filters to search specific parts of the document...\")\n",
    "\n",
    "# First, let's see what metadata is available\n",
    "print(\"\\nAvailable metadata in our chunks:\")\n",
    "if chunks:\n",
    "    sample_metadata = chunks[0].metadata\n",
    "    print(f\"Sample metadata: {sample_metadata}\")\n",
    "    \n",
    "    # Get unique page numbers for filtering examples\n",
    "    page_numbers = set()\n",
    "    for chunk in chunks[:10]:  # Check first 10 chunks\n",
    "        if 'page' in chunk.metadata:\n",
    "            page_numbers.add(chunk.metadata['page'])\n",
    "    print(f\"Available page numbers (sample): {sorted(list(page_numbers))[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.3.1 Filter by Specific Page\n",
      "Searching only in Page 0:\n",
      "  Result 1: Page 0 - Chapter 4. Working with Docker Images\n",
      "Every Linux container is based on an image. Images are the underlying definition of what\n",
      "gets reconstituted into...\n",
      "  Result 2: Page 0 - with the Dockerfile. This file describes all the steps that are required to create an image\n",
      "and is usually contained within the root directory of the ...\n",
      "  Result 3: Page 0 - image management, Docker relies heavily on this storage backend, which communicates\n",
      "with the underlying Linux filesystem to build and manage the multi...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5.3.1 Filter by Specific Page\")\n",
    "if page_numbers:\n",
    "    target_page = sorted(list(page_numbers))[0]  # Use first available page\n",
    "    page_results = vector_store.similarity_search(\n",
    "        \"methodology approach\",\n",
    "        k=10,\n",
    "        filter={\"page\": target_page}\n",
    "    )\n",
    "    print(f\"Searching only in Page {target_page}:\")\n",
    "    for i, doc in enumerate(page_results, 1):\n",
    "        print(f\"  Result {i}: Page {doc.metadata.get('page')} - {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.3.3 Multiple Metadata Filters\n",
      "Using complex filter (page >= 0 AND has source):\n",
      "  Result 1: Page 6 - age. In many cases, you will simply see a . at the end of a build command, since a single\n",
      "period represents the current directory. This build context ...\n",
      "  Result 2: Page 0 - Chapter 4. Working with Docker Images\n",
      "Every Linux container is based on an image. Images are the underlying definition of what\n",
      "gets reconstituted into...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5.3.3 Multiple Metadata Filters\")\n",
    "# Complex filtering with multiple conditions\n",
    "complex_results = vector_store.similarity_search(\n",
    "    \"research findings\",\n",
    "    k=2,\n",
    "    filter={\n",
    "        \"$and\": [\n",
    "            {\"page\": {\"$lte\": 10}},  # Page 0 or higher\n",
    "            {\"source\": {\"$ne\": \"\"}}  # Has a source\n",
    "        ]\n",
    "    } # type: ignore\n",
    ")\n",
    "\n",
    "print(\"Using complex filter (page >= 0 AND has source):\")\n",
    "for i, doc in enumerate(complex_results, 1):\n",
    "    print(f\"  Result {i}: Page {doc.metadata.get('page')} - {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Creating Retriever...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: RETRIEVERS\n",
    "# ============================================================================\n",
    "print(\"\\n6. Creating Retriever...\")\n",
    "\n",
    "# Similarity Retriever\n",
    "similarity_retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'What are the main parts of building a docker image?'\n",
      "✓ Retrieved 4 relevant document chunks\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: RAG FOUNDATION\n",
    "# ============================================================================\n",
    "# final_query = \"What are the key contributions of this paper?\"\n",
    "final_query = \"What are the main parts of building a docker image?\"\n",
    "context_docs = similarity_retriever.invoke(final_query)\n",
    "\n",
    "print(f\"\\nQuery: '{final_query}'\")\n",
    "print(f\"✓ Retrieved {len(context_docs)} relevant document chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context that would be sent to LLM:\n",
      "\n",
      "Chunk 1: with the Dockerfile. This file describes all the steps that are required to create an image\n",
      "and is usually contained within the root directory of the source code repository for your\n",
      "application.\n",
      "1...\n",
      "\n",
      "Chunk 2: Chapter 4. Working with Docker Images\n",
      "Every Linux container is based on an image. Images are the underlying definition of what\n",
      "gets reconstituted into a running container, much like a virtual disk becomes a VM when\n",
      "you start it up. Docker or Open Con...\n"
     ]
    }
   ],
   "source": [
    "# Show what would be sent to LLM\n",
    "print(f\"\\nContext that would be sent to LLM:\")\n",
    "for i, doc in enumerate(context_docs[:2], 1):  # Show first 2 for brevity\n",
    "    print(f\"\\nChunk {i}: {doc.page_content[:250]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
