{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6326edd",
   "metadata": {},
   "source": [
    "## RAG with Langgraph Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4979cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the langchain pre-release for latest features\n",
    "# pip install --pre -U langchain langchain-ollama, langchain-chroma, langchain-core langchain-community\n",
    "\n",
    "# for stable version once v1 is released\n",
    "# pip install langchain langchain-ollama, langchain-chroma, langchain-core langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fd2b4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Core imports\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('./../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9db37bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to existing Chroma vector store...\n",
      "‚úì Connected to existing vector store\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: CONNECT TO EXISTING VECTOR STORE\n",
    "# ============================================================================\n",
    "print(\"Connecting to existing Chroma vector store...\")\n",
    "\n",
    "# Use the same embeddings as your ingestion code\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "# Chroma Vector Store (assumes data already exists from previous code)\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"pdf_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"../01-semantic-search/chroma_db\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Connected to existing vector store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14912208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing Vector Store Connection...\n",
      "‚úì Collection 'pdf_collection' found with 270 documents\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç Testing Vector Store Connection...\")\n",
    "\n",
    "# Test 1: Check collection info\n",
    "collection = vector_store._collection\n",
    "doc_count = collection.count()\n",
    "print(f\"‚úì Collection '{collection.name}' found with {doc_count} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "481e4b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created retriever tool\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: CREATE RETRIEVER TOOL\n",
    "# ============================================================================\n",
    "@tool\n",
    "def retrieve_documents(query: str):\n",
    "    \"\"\"Search and return information from the PDF documents in the database.\"\"\"\n",
    "    print(f\"üîç Searching: '{query}'\")\n",
    "    \n",
    "    # Perform similarity search using the retriever\n",
    "    docs = vector_store.similarity_search(query,k=4)\n",
    "    \n",
    "    # Format results for the LLM\n",
    "    if docs:\n",
    "        content = \"\\n\\n\".join(\n",
    "            f\"Page {doc.metadata.get('page', '?')}: {doc.page_content}\" \n",
    "            for doc in docs\n",
    "        )\n",
    "        print(f\"‚úì Found {len(docs)} relevant chunks\")\n",
    "        return content\n",
    "    else:\n",
    "        print(\"‚úó No relevant documents found\")\n",
    "        return \"No relevant information found.\"\n",
    "\n",
    "retriever_tool = retrieve_documents\n",
    "\n",
    "print(\"‚úì Created retriever tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c538b705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Initialized chat models\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: INITIALIZE MODELS\n",
    "# ============================================================================\n",
    "# Using ChatOllama instead of OpenAI\n",
    "response_model = ChatOllama(\n",
    "    model=\"llama3.2:3b\", \n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0\n",
    ")\n",
    "grader_model = ChatOllama(\n",
    "    model=\"llama3.2:3b\", \n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"‚úì Initialized chat models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99d810ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: NODE FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_query_or_respond(state: MessagesState):\n",
    "    \"\"\"Call the model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n",
    "    \"\"\"\n",
    "    response = (\n",
    "        response_model\n",
    "        .bind_tools([retriever_tool]).invoke(state[\"messages\"])  \n",
    "    )\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82d98802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document grading\n",
    "GRADE_PROMPT = (\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n",
    "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
    "    \"Here is the user question: {question} \\n\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n\"\n",
    "    \"Only respond with 'yes' or 'no'.\"\n",
    ")\n",
    "\n",
    "def grade_documents(\n",
    "    state: MessagesState,\n",
    ") -> Literal[\"generate_answer\", \"rewrite_question\"]:\n",
    "    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "\n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = grader_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "    # Extract the score from the response content\n",
    "    # Handle both string and list content types\n",
    "    content = response.content\n",
    "    if isinstance(content, list):\n",
    "        # If content is a list, join it or take the first element\n",
    "        score = \" \".join(str(item) for item in content).strip().lower()\n",
    "    else:\n",
    "        score = str(content).strip().lower()\n",
    "    \n",
    "    # Ensure we only get 'yes' or 'no'\n",
    "    if \"yes\" in score:\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        return \"rewrite_question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97f1da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question rewriting\n",
    "REWRITE_PROMPT = (\n",
    "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
    "    \"Here is the initial question:\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"Formulate an improved question:\"\n",
    ")\n",
    "\n",
    "def rewrite_question(state: MessagesState):\n",
    "    \"\"\"Rewrite the original user question.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54dec06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer generation\n",
    "GENERATE_PROMPT = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Question: {question} \\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "def generate_answer(state: MessagesState):\n",
    "    \"\"\"Generate an answer.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c69fb64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Assembled and compiled the graph\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: ASSEMBLE THE GRAPH\n",
    "# ============================================================================\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(generate_query_or_respond)\n",
    "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
    "workflow.add_node(rewrite_question)\n",
    "workflow.add_node(generate_answer)\n",
    "\n",
    "workflow.add_edge(START, \"generate_query_or_respond\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_query_or_respond\",\n",
    "    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "print(\"‚úì Assembled and compiled the graph\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8187cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x11317e250>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d56d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: TEST THE AGENTIC RAG\n",
    "# ============================================================================\n",
    "def run_agentic_rag(question: str):\n",
    "    \"\"\"Run the agentic RAG system with a question\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # MessagesState accepts shorthand message format via add_messages reducer\n",
    "    for chunk in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": question}]}):  # type: ignore\n",
    "        for node, update in chunk.items():\n",
    "            print(f\"\\nUpdate from node: {node}\")\n",
    "            print(\"-\" * 40)\n",
    "            print(update[\"messages\"][-1])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8023989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUESTION: What are the main methods mentioned in this paper?\n",
      "============================================================\n",
      "\n",
      "Update from node: generate_query_or_respond\n",
      "----------------------------------------\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-24T20:18:21.173892Z', 'done': True, 'done_reason': 'stop', 'total_duration': 15807218955, 'load_duration': 7338167498, 'prompt_eval_count': 164, 'prompt_eval_duration': 7124845757, 'eval_count': 20, 'eval_duration': 1166666155, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'} id='lc_run--9d327291-653b-4f7e-8734-fcbbc3168259-0' tool_calls=[{'name': 'retrieve_documents', 'args': {'query': 'main methods in paper'}, 'id': '516cd0d4-18b3-4457-88bd-af8bd60c2805', 'type': 'tool_call'}] usage_metadata={'input_tokens': 164, 'output_tokens': 20, 'total_tokens': 184}\n",
      "\n",
      "üîç Searching: 'main methods in paper'\n",
      "‚úì Found 4 relevant chunks\n",
      "\n",
      "Update from node: retrieve\n",
      "----------------------------------------\n",
      "content='Page 0: Chapter 4. Working with Docker Images\\nEvery Linux container is based on an image. Images are the underlying definition of what\\ngets reconstituted into a running container, much like a virtual disk becomes a VM when\\nyou start it up. Docker or Open Container Initiative (OCI) images provide the basis for\\neverything that you will ever deploy and run with Docker. To launch a container, you must\\neither download a public image or create your own. You can think of the image as a single\\nasset that primarily represents the filesystem for the container. However, in reality, every\\nimage consists of one or more linked filesystem layers that generally have a direct one-to-\\none mapping to each build step used to create that image.\\nBecause images are built up from individual layers, they put special demands on the Linux\\nkernel, which must provide the drivers that Docker needs to run the storage backend. For\\nimage management, Docker relies heavily on this storage backend, which communicates\\n\\nPage 0: Chapter 4. Working with Docker Images\\nEvery Linux container is based on an image. Images are the underlying definition of what\\ngets reconstituted into a running container, much like a virtual disk becomes a VM when\\nyou start it up. Docker or Open Container Initiative (OCI) images provide the basis for\\neverything that you will ever deploy and run with Docker. To launch a container, you must\\neither download a public image or create your own. You can think of the image as a single\\nasset that primarily represents the filesystem for the container. However, in reality, every\\nimage consists of one or more linked filesystem layers that generally have a direct one-to-\\none mapping to each build step used to create that image.\\nBecause images are built up from individual layers, they put special demands on the Linux\\nkernel, which must provide the drivers that Docker needs to run the storage backend. For\\nimage management, Docker relies heavily on this storage backend, which communicates\\n\\nPage 0: image management, Docker relies heavily on this storage backend, which communicates\\nwith the underlying Linux filesystem to build and manage the multiple layers that com-\\nbine into a single usable image. The primary storage backends that are supported include\\nthe following:\\nOverlay2\\nB-Tree File System (Btrfs)\\nDevice Mapper\\nEach storage backend provides a fast copy-on-write (CoW) system for image management.\\nWe discuss the specifics of various backends in Chapter\\xa011. For now, we‚Äôll use the default\\nbackend and explore how images work, since they make up the basis for almost every-\\nthing else that you will do with Docker, including the following:\\nBuilding images\\nUploading (pushing) images to an image registry\\nDownloading (pulling) images from an image registry\\nCreating and running containers from an image\\nAnatomy of a Dockerfile\\nTo create a custom Docker image with the default tools, you will need to become familiar\\nwith the Dockerfile. This file describes all the steps that are required to create an image\\n\\nPage 0: image management, Docker relies heavily on this storage backend, which communicates\\nwith the underlying Linux filesystem to build and manage the multiple layers that com-\\nbine into a single usable image. The primary storage backends that are supported include\\nthe following:\\nOverlay2\\nB-Tree File System (Btrfs)\\nDevice Mapper\\nEach storage backend provides a fast copy-on-write (CoW) system for image management.\\nWe discuss the specifics of various backends in Chapter\\xa011. For now, we‚Äôll use the default\\nbackend and explore how images work, since they make up the basis for almost every-\\nthing else that you will do with Docker, including the following:\\nBuilding images\\nUploading (pushing) images to an image registry\\nDownloading (pulling) images from an image registry\\nCreating and running containers from an image\\nAnatomy of a Dockerfile\\nTo create a custom Docker image with the default tools, you will need to become familiar\\nwith the Dockerfile. This file describes all the steps that are required to create an image' name='retrieve_documents' id='ff9500ad-5580-4ab8-8c91-aa5b4ce04491' tool_call_id='516cd0d4-18b3-4457-88bd-af8bd60c2805'\n",
      "\n",
      "\n",
      "Update from node: rewrite_question\n",
      "----------------------------------------\n",
      "{'role': 'user', 'content': 'Based on the input, it appears that the user is asking for a summary of the main methods or techniques discussed in the paper. To improve the question, I would suggest:\\n\\n\"What are the key methodologies and approaches presented in this research paper?\"\\n\\nThis revised question better captures the semantic intent behind the original query, focusing on the specific methods and techniques used in the paper rather than just listing them as \"main methods\".'}\n",
      "\n",
      "\n",
      "Update from node: generate_query_or_respond\n",
      "----------------------------------------\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-24T20:19:54.673512Z', 'done': True, 'done_reason': 'stop', 'total_duration': 43740998220, 'load_duration': 121910402, 'prompt_eval_count': 1117, 'prompt_eval_duration': 41709471265, 'eval_count': 25, 'eval_duration': 1668818369, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'} id='lc_run--a3bc162c-b6af-4e23-b53f-7850aede6c96-0' tool_calls=[{'name': 'retrieve_documents', 'args': {'query': 'key methodologies and approaches presented in this research paper'}, 'id': 'ec4faf0b-5df3-42dd-918b-69fdf6d14c5a', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1117, 'output_tokens': 25, 'total_tokens': 1142}\n",
      "\n",
      "üîç Searching: 'key methodologies and approaches presented in this research paper'\n",
      "‚úì Found 4 relevant chunks\n",
      "\n",
      "Update from node: retrieve\n",
      "----------------------------------------\n",
      "content='Page 0: Chapter 4. Working with Docker Images\\nEvery Linux container is based on an image. Images are the underlying definition of what\\ngets reconstituted into a running container, much like a virtual disk becomes a VM when\\nyou start it up. Docker or Open Container Initiative (OCI) images provide the basis for\\neverything that you will ever deploy and run with Docker. To launch a container, you must\\neither download a public image or create your own. You can think of the image as a single\\nasset that primarily represents the filesystem for the container. However, in reality, every\\nimage consists of one or more linked filesystem layers that generally have a direct one-to-\\none mapping to each build step used to create that image.\\nBecause images are built up from individual layers, they put special demands on the Linux\\nkernel, which must provide the drivers that Docker needs to run the storage backend. For\\nimage management, Docker relies heavily on this storage backend, which communicates\\n\\nPage 0: Chapter 4. Working with Docker Images\\nEvery Linux container is based on an image. Images are the underlying definition of what\\ngets reconstituted into a running container, much like a virtual disk becomes a VM when\\nyou start it up. Docker or Open Container Initiative (OCI) images provide the basis for\\neverything that you will ever deploy and run with Docker. To launch a container, you must\\neither download a public image or create your own. You can think of the image as a single\\nasset that primarily represents the filesystem for the container. However, in reality, every\\nimage consists of one or more linked filesystem layers that generally have a direct one-to-\\none mapping to each build step used to create that image.\\nBecause images are built up from individual layers, they put special demands on the Linux\\nkernel, which must provide the drivers that Docker needs to run the storage backend. For\\nimage management, Docker relies heavily on this storage backend, which communicates\\n\\nPage 13: to deploy an application. Internet latencies have a very real impact on software deploy-\\nments, and outages that affect these registries could have a very detrimental impact on a\\ncompany‚Äôs ability to deploy smoothly and on schedule. This is mitigated by good image de-\\nsign, where you make thin layers that are easy to move around the internet.\\nPrivate Registries\\nThe other option that many companies consider is to host some type of Docker image reg-\\nistry internally, which can interact with the Docker client to support pushing, pulling, and\\nsearching images. The open source Distribution project provides the basic functionality\\nthat most other registries build upon.\\nOther strong contenders in the private registry space include Harbor and Red Hat Quay. In\\naddition to the basic Docker registry functionality, these products have solid GUI interfaces\\nand many additional features, like image verification.\\nAuthenticating to a Registry\\n\\nPage 13: to deploy an application. Internet latencies have a very real impact on software deploy-\\nments, and outages that affect these registries could have a very detrimental impact on a\\ncompany‚Äôs ability to deploy smoothly and on schedule. This is mitigated by good image de-\\nsign, where you make thin layers that are easy to move around the internet.\\nPrivate Registries\\nThe other option that many companies consider is to host some type of Docker image reg-\\nistry internally, which can interact with the Docker client to support pushing, pulling, and\\nsearching images. The open source Distribution project provides the basic functionality\\nthat most other registries build upon.\\nOther strong contenders in the private registry space include Harbor and Red Hat Quay. In\\naddition to the basic Docker registry functionality, these products have solid GUI interfaces\\nand many additional features, like image verification.\\nAuthenticating to a Registry' name='retrieve_documents' id='fa132b7a-d17f-46ec-9f2f-e33e61ef3237' tool_call_id='ec4faf0b-5df3-42dd-918b-69fdf6d14c5a'\n",
      "\n",
      "\n",
      "Update from node: rewrite_question\n",
      "----------------------------------------\n",
      "{'role': 'user', 'content': 'Based on the input, it appears that the user is asking for a summary of the main methods or techniques discussed in the paper. To improve the question, I would suggest:\\n\\n\"What are the key methodologies and approaches presented in this research paper?\"\\n\\nThis revised question better captures the semantic intent behind the original query, focusing on the specific methods and techniques used in the paper rather than just listing them as \"main methods\".'}\n",
      "\n",
      "\n",
      "Update from node: generate_query_or_respond\n",
      "----------------------------------------\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-24T20:21:58.208145Z', 'done': True, 'done_reason': 'stop', 'total_duration': 77586762300, 'load_duration': 126733115, 'prompt_eval_count': 2017, 'prompt_eval_duration': 75246124048, 'eval_count': 25, 'eval_duration': 1926678812, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'} id='lc_run--3ef3a587-7805-4c71-a668-cb4b0e96b11e-0' tool_calls=[{'name': 'retrieve_documents', 'args': {'query': 'key methodologies and approaches presented in this research paper'}, 'id': '4c7fab85-4aa5-45c1-8453-2737ebd5fa0b', 'type': 'tool_call'}] usage_metadata={'input_tokens': 2017, 'output_tokens': 25, 'total_tokens': 2042}\n",
      "\n",
      "üîç Searching: 'key methodologies and approaches presented in this research paper'\n",
      "‚úì Found 4 relevant chunks\n",
      "\n",
      "Update from node: retrieve\n",
      "----------------------------------------\n",
      "content='Page 0: Chapter 4. Working with Docker Images\\nEvery Linux container is based on an image. Images are the underlying definition of what\\ngets reconstituted into a running container, much like a virtual disk becomes a VM when\\nyou start it up. Docker or Open Container Initiative (OCI) images provide the basis for\\neverything that you will ever deploy and run with Docker. To launch a container, you must\\neither download a public image or create your own. You can think of the image as a single\\nasset that primarily represents the filesystem for the container. However, in reality, every\\nimage consists of one or more linked filesystem layers that generally have a direct one-to-\\none mapping to each build step used to create that image.\\nBecause images are built up from individual layers, they put special demands on the Linux\\nkernel, which must provide the drivers that Docker needs to run the storage backend. For\\nimage management, Docker relies heavily on this storage backend, which communicates\\n\\nPage 0: Chapter 4. Working with Docker Images\\nEvery Linux container is based on an image. Images are the underlying definition of what\\ngets reconstituted into a running container, much like a virtual disk becomes a VM when\\nyou start it up. Docker or Open Container Initiative (OCI) images provide the basis for\\neverything that you will ever deploy and run with Docker. To launch a container, you must\\neither download a public image or create your own. You can think of the image as a single\\nasset that primarily represents the filesystem for the container. However, in reality, every\\nimage consists of one or more linked filesystem layers that generally have a direct one-to-\\none mapping to each build step used to create that image.\\nBecause images are built up from individual layers, they put special demands on the Linux\\nkernel, which must provide the drivers that Docker needs to run the storage backend. For\\nimage management, Docker relies heavily on this storage backend, which communicates\\n\\nPage 13: to deploy an application. Internet latencies have a very real impact on software deploy-\\nments, and outages that affect these registries could have a very detrimental impact on a\\ncompany‚Äôs ability to deploy smoothly and on schedule. This is mitigated by good image de-\\nsign, where you make thin layers that are easy to move around the internet.\\nPrivate Registries\\nThe other option that many companies consider is to host some type of Docker image reg-\\nistry internally, which can interact with the Docker client to support pushing, pulling, and\\nsearching images. The open source Distribution project provides the basic functionality\\nthat most other registries build upon.\\nOther strong contenders in the private registry space include Harbor and Red Hat Quay. In\\naddition to the basic Docker registry functionality, these products have solid GUI interfaces\\nand many additional features, like image verification.\\nAuthenticating to a Registry\\n\\nPage 13: to deploy an application. Internet latencies have a very real impact on software deploy-\\nments, and outages that affect these registries could have a very detrimental impact on a\\ncompany‚Äôs ability to deploy smoothly and on schedule. This is mitigated by good image de-\\nsign, where you make thin layers that are easy to move around the internet.\\nPrivate Registries\\nThe other option that many companies consider is to host some type of Docker image reg-\\nistry internally, which can interact with the Docker client to support pushing, pulling, and\\nsearching images. The open source Distribution project provides the basic functionality\\nthat most other registries build upon.\\nOther strong contenders in the private registry space include Harbor and Red Hat Quay. In\\naddition to the basic Docker registry functionality, these products have solid GUI interfaces\\nand many additional features, like image verification.\\nAuthenticating to a Registry' name='retrieve_documents' id='72d1e8e8-c286-4f15-8817-c1fa0713d39f' tool_call_id='4c7fab85-4aa5-45c1-8453-2737ebd5fa0b'\n",
      "\n",
      "\n",
      "Update from node: rewrite_question\n",
      "----------------------------------------\n",
      "{'role': 'user', 'content': 'Based on the input, it appears that the user is asking for a summary of the main methods or techniques discussed in the paper. To improve the question, I would suggest:\\n\\n\"What are the key methodologies and approaches presented in this research paper?\"\\n\\nThis revised question better captures the semantic intent behind the original query, focusing on the specific methods and techniques used in the paper rather than just listing them as \"main methods\".'}\n",
      "\n",
      "\n",
      "Update from node: generate_query_or_respond\n",
      "----------------------------------------\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-24T20:24:35.094711Z', 'done': True, 'done_reason': 'stop', 'total_duration': 111358812950, 'load_duration': 121017261, 'prompt_eval_count': 2917, 'prompt_eval_duration': 108956090812, 'eval_count': 25, 'eval_duration': 1953067456, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'} id='lc_run--7721ec0c-d53c-483d-8e95-06406ae1b6c8-0' tool_calls=[{'name': 'retrieve_documents', 'args': {'query': 'key methodologies and approaches presented in this research paper'}, 'id': '251c6f35-8b9f-463f-b070-2e9ee0eac5ff', 'type': 'tool_call'}] usage_metadata={'input_tokens': 2917, 'output_tokens': 25, 'total_tokens': 2942}\n",
      "\n",
      "üîç Searching: 'key methodologies and approaches presented in this research paper'\n",
      "‚úì Found 4 relevant chunks\n",
      "\n",
      "Update from node: retrieve\n",
      "----------------------------------------\n",
      "content='Page 0: Chapter 4. Working with Docker Images\\nEvery Linux container is based on an image. Images are the underlying definition of what\\ngets reconstituted into a running container, much like a virtual disk becomes a VM when\\nyou start it up. Docker or Open Container Initiative (OCI) images provide the basis for\\neverything that you will ever deploy and run with Docker. To launch a container, you must\\neither download a public image or create your own. You can think of the image as a single\\nasset that primarily represents the filesystem for the container. However, in reality, every\\nimage consists of one or more linked filesystem layers that generally have a direct one-to-\\none mapping to each build step used to create that image.\\nBecause images are built up from individual layers, they put special demands on the Linux\\nkernel, which must provide the drivers that Docker needs to run the storage backend. For\\nimage management, Docker relies heavily on this storage backend, which communicates\\n\\nPage 0: Chapter 4. Working with Docker Images\\nEvery Linux container is based on an image. Images are the underlying definition of what\\ngets reconstituted into a running container, much like a virtual disk becomes a VM when\\nyou start it up. Docker or Open Container Initiative (OCI) images provide the basis for\\neverything that you will ever deploy and run with Docker. To launch a container, you must\\neither download a public image or create your own. You can think of the image as a single\\nasset that primarily represents the filesystem for the container. However, in reality, every\\nimage consists of one or more linked filesystem layers that generally have a direct one-to-\\none mapping to each build step used to create that image.\\nBecause images are built up from individual layers, they put special demands on the Linux\\nkernel, which must provide the drivers that Docker needs to run the storage backend. For\\nimage management, Docker relies heavily on this storage backend, which communicates\\n\\nPage 13: to deploy an application. Internet latencies have a very real impact on software deploy-\\nments, and outages that affect these registries could have a very detrimental impact on a\\ncompany‚Äôs ability to deploy smoothly and on schedule. This is mitigated by good image de-\\nsign, where you make thin layers that are easy to move around the internet.\\nPrivate Registries\\nThe other option that many companies consider is to host some type of Docker image reg-\\nistry internally, which can interact with the Docker client to support pushing, pulling, and\\nsearching images. The open source Distribution project provides the basic functionality\\nthat most other registries build upon.\\nOther strong contenders in the private registry space include Harbor and Red Hat Quay. In\\naddition to the basic Docker registry functionality, these products have solid GUI interfaces\\nand many additional features, like image verification.\\nAuthenticating to a Registry\\n\\nPage 13: to deploy an application. Internet latencies have a very real impact on software deploy-\\nments, and outages that affect these registries could have a very detrimental impact on a\\ncompany‚Äôs ability to deploy smoothly and on schedule. This is mitigated by good image de-\\nsign, where you make thin layers that are easy to move around the internet.\\nPrivate Registries\\nThe other option that many companies consider is to host some type of Docker image reg-\\nistry internally, which can interact with the Docker client to support pushing, pulling, and\\nsearching images. The open source Distribution project provides the basic functionality\\nthat most other registries build upon.\\nOther strong contenders in the private registry space include Harbor and Red Hat Quay. In\\naddition to the basic Docker registry functionality, these products have solid GUI interfaces\\nand many additional features, like image verification.\\nAuthenticating to a Registry' name='retrieve_documents' id='2883de94-f26f-434b-b3ac-d0e43cea06ef' tool_call_id='251c6f35-8b9f-463f-b070-2e9ee0eac5ff'\n",
      "\n",
      "\n",
      "Update from node: rewrite_question\n",
      "----------------------------------------\n",
      "{'role': 'user', 'content': 'Based on the input, it appears that the user is asking for a summary of the main methods or techniques discussed in the paper. To improve the question, I would suggest:\\n\\n\"What are the key methodologies and approaches presented in this research paper?\"\\n\\nThis revised question better captures the semantic intent behind the original query, focusing on the specific methods and techniques used in the paper rather than just listing them as \"main methods\".'}\n",
      "\n",
      "\n",
      "Update from node: generate_query_or_respond\n",
      "----------------------------------------\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-24T20:27:51.998576Z', 'done': True, 'done_reason': 'stop', 'total_duration': 152324107622, 'load_duration': 124666471, 'prompt_eval_count': 3817, 'prompt_eval_duration': 149393749082, 'eval_count': 25, 'eval_duration': 2415968592, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'} id='lc_run--b5ba5129-a1ef-4c6d-af04-159b4bd5fa12-0' tool_calls=[{'name': 'retrieve_documents', 'args': {'query': 'key methodologies and approaches presented in this research paper'}, 'id': '87edaacd-e7a0-465a-b989-67875c549cd7', 'type': 'tool_call'}] usage_metadata={'input_tokens': 3817, 'output_tokens': 25, 'total_tokens': 3842}\n",
      "\n",
      "üîç Searching: 'key methodologies and approaches presented in this research paper'\n",
      "‚úì Found 4 relevant chunks\n",
      "\n",
      "Update from node: retrieve\n",
      "----------------------------------------\n",
      "content='Page 0: Chapter 4. Working with Docker Images\\nEvery Linux container is based on an image. Images are the underlying definition of what\\ngets reconstituted into a running container, much like a virtual disk becomes a VM when\\nyou start it up. Docker or Open Container Initiative (OCI) images provide the basis for\\neverything that you will ever deploy and run with Docker. To launch a container, you must\\neither download a public image or create your own. You can think of the image as a single\\nasset that primarily represents the filesystem for the container. However, in reality, every\\nimage consists of one or more linked filesystem layers that generally have a direct one-to-\\none mapping to each build step used to create that image.\\nBecause images are built up from individual layers, they put special demands on the Linux\\nkernel, which must provide the drivers that Docker needs to run the storage backend. For\\nimage management, Docker relies heavily on this storage backend, which communicates\\n\\nPage 0: Chapter 4. Working with Docker Images\\nEvery Linux container is based on an image. Images are the underlying definition of what\\ngets reconstituted into a running container, much like a virtual disk becomes a VM when\\nyou start it up. Docker or Open Container Initiative (OCI) images provide the basis for\\neverything that you will ever deploy and run with Docker. To launch a container, you must\\neither download a public image or create your own. You can think of the image as a single\\nasset that primarily represents the filesystem for the container. However, in reality, every\\nimage consists of one or more linked filesystem layers that generally have a direct one-to-\\none mapping to each build step used to create that image.\\nBecause images are built up from individual layers, they put special demands on the Linux\\nkernel, which must provide the drivers that Docker needs to run the storage backend. For\\nimage management, Docker relies heavily on this storage backend, which communicates\\n\\nPage 13: to deploy an application. Internet latencies have a very real impact on software deploy-\\nments, and outages that affect these registries could have a very detrimental impact on a\\ncompany‚Äôs ability to deploy smoothly and on schedule. This is mitigated by good image de-\\nsign, where you make thin layers that are easy to move around the internet.\\nPrivate Registries\\nThe other option that many companies consider is to host some type of Docker image reg-\\nistry internally, which can interact with the Docker client to support pushing, pulling, and\\nsearching images. The open source Distribution project provides the basic functionality\\nthat most other registries build upon.\\nOther strong contenders in the private registry space include Harbor and Red Hat Quay. In\\naddition to the basic Docker registry functionality, these products have solid GUI interfaces\\nand many additional features, like image verification.\\nAuthenticating to a Registry\\n\\nPage 13: to deploy an application. Internet latencies have a very real impact on software deploy-\\nments, and outages that affect these registries could have a very detrimental impact on a\\ncompany‚Äôs ability to deploy smoothly and on schedule. This is mitigated by good image de-\\nsign, where you make thin layers that are easy to move around the internet.\\nPrivate Registries\\nThe other option that many companies consider is to host some type of Docker image reg-\\nistry internally, which can interact with the Docker client to support pushing, pulling, and\\nsearching images. The open source Distribution project provides the basic functionality\\nthat most other registries build upon.\\nOther strong contenders in the private registry space include Harbor and Red Hat Quay. In\\naddition to the basic Docker registry functionality, these products have solid GUI interfaces\\nand many additional features, like image verification.\\nAuthenticating to a Registry' name='retrieve_documents' id='dea159e7-e63d-453b-b0f6-3d42bb0cd760' tool_call_id='87edaacd-e7a0-465a-b989-67875c549cd7'\n",
      "\n",
      "\n",
      "Update from node: rewrite_question\n",
      "----------------------------------------\n",
      "{'role': 'user', 'content': 'Based on the input, it appears that the user is asking for a summary of the main methods or techniques discussed in the paper. To improve the question, I would suggest:\\n\\n\"What are the key methodologies and approaches presented in this research paper?\"\\n\\nThis revised question better captures the semantic intent behind the original query, focusing on the specific methods and techniques used in the paper rather than just listing them as \"main methods\".'}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mWhat are the main methods mentioned in this paper?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mrun_agentic_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mrun_agentic_rag\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# MessagesState accepts shorthand message format via add_messages reducer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mUpdate from node: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/langgraph/pregel/main.py:2633\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2631\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2632\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2633\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2640\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2643\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mgenerate_query_or_respond\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_query_or_respond\u001b[39m(state: MessagesState):\n\u001b[32m      6\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model to generate a response based on the current state. Given\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m     response = (\n\u001b[32m     10\u001b[39m         \u001b[43mresponse_model\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mretriever_tool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m     12\u001b[39m     )\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [response]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:5534\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5527\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5528\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5529\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5532\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5533\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5535\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5536\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5537\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5538\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:1025\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1019\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1020\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> ChatResult:\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1028\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m   1029\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m   1030\u001b[39m         message=AIMessage(\n\u001b[32m   1031\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1038\u001b[39m         generation_info=generation_info,\n\u001b[32m   1039\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:960\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    952\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    953\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    957\u001b[39m     **kwargs: Any,\n\u001b[32m    958\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    959\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:1049\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m   1043\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1044\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   1045\u001b[39m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1046\u001b[39m     **kwargs: Any,\n\u001b[32m   1047\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m   1048\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1055\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:947\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    945\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    946\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    948\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m    949\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/ollama/_client.py:174\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m      \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/httpx/_client.py:868\u001b[39m, in \u001b[36mClient.stream\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    845\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[33;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[33;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    853\u001b[39m \u001b[33;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[32m    854\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    855\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    856\u001b[39m     method=method,\n\u001b[32m    857\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    866\u001b[39m     extensions=extensions,\n\u001b[32m    867\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    875\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self-learning/langchain-v1-projects/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "question = \"What are the main methods mentioned in this paper?\"\n",
    "run_agentic_rag(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8e303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUESTION: Tell me about earth\n",
      "============================================================\n",
      "\n",
      "Update from node: generate_query_or_respond\n",
      "----------------------------------------\n",
      "content='<think>\\nOkay, the user asked \"Tell me about earth.\" I need to figure out how to respond. Let me check the tools available. There\\'s a function called retrieve_documents that searches PDF documents in the database. The function requires a query parameter. Since the user is asking about Earth, I should use this function to get information from the documents. The query parameter should be \"Earth\" or \"about Earth.\" Let me call the function with \"Earth\" as the query. That should retrieve the relevant information from the PDFs. I\\'ll structure the tool call accordingly.\\n</think>\\n\\n' additional_kwargs={} response_metadata={'model': 'qwen3', 'created_at': '2025-09-21T17:04:20.7535156Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1001018200, 'load_duration': 48519900, 'prompt_eval_count': 146, 'prompt_eval_duration': 53650200, 'eval_count': 138, 'eval_duration': 898272900, 'model_name': 'qwen3'} id='run--827ce29e-346b-4d8d-a5a2-a3ecabd7e7ca-0' tool_calls=[{'type': 'tool_call', 'id': '9eeb2952-cd32-4b5a-ad72-45cdffdde407', 'name': 'retrieve_documents', 'args': {'query': 'Earth'}}] usage_metadata={'input_tokens': 146, 'output_tokens': 138, 'total_tokens': 284}\n",
      "\n",
      "üîç Searching: 'Earth'\n",
      "‚úì Found 4 relevant chunks\n",
      "\n",
      "Update from node: retrieve\n",
      "----------------------------------------\n",
      "content='Page 99: The environment is the world where the task is executed, which can be set up as the LLM\\nitself or an external system, e.g., a simulator or a virtual world like Minecraft [359, 337]. The\\nenvironment provides feedback to the task planner about the result of the actions, which can\\nbe used to update the plan, either in the form of natural language or from other multimodal\\nsignals [322, 301]\\n4.4.3 Plan generation\\nFor solving complex tasks, the planner needs to generate a long-term and multi-step plan, which\\nrequires the planner to be able to reason over long-term dependencies and develop a coherent\\nand consistent plan. First, it needs to understand the task and break it down into sub-tasks,\\nthen generate a plan that can accomplish the task by executing the sub-tasks in a proper order.\\nThe plan should be generated in an interpretable and executable way by the executor, which\\nacts according to the plan and interacts with the environment to accomplish the task. The\\n\\nPage 173: 2024-04-14.\\n[391] Common Crawl. https://commoncrawl.org/. Accessed: 2024-04-15.\\n[392] Project Gutenberg. https://www.gutenberg.org/. Accessed: 2024-04-14.\\n[393] Wikipedia. https://en.wikipedia.org/wiki/Main_Page. Accessed: 2024-04-14.\\n174\\n\\nPage 17: the top 500 students in the US in a qualifier for the USA Math Olympiad (AIME), and exceeds human PhD-level\\naccuracy on a benchmark of physics, biology, and chemistry problems (GPQA).\\n20Claude 3.5 Sonnet and Gemini\\n21adversarial prompts that purposely try to circumvent model refusals for content it‚Äôs not supposed to produce\\n18\\n\\nPage 95: Taylor need to do before this? A. get a certificate , B. teach\\nsmall children, C. work in a school\\nARC Science Choose your answer to the question: Which technology was\\ndeveloped most recently? A. cellular telephone, B. television,\\nC. refrigerator, D. airplane\\nQASC Science Choose your answer to the question: What is described in terms\\nof temperature and water in the air? A. storms; B. climate;\\nC. mass; D. seasonal; E. winter; F. density; G. length\\nHellaSWAG Event Choose your answer to the question: We see a chair with a pillow\\non it. A. a man holding a cat does curling. B. a man holding a\\ncat starts hitting objects on an item. C. a man holding a cat is\\nwrapping a box. D. a man holding a cat sits down on the\\nchair.\\nNumerSense Numerical a square is a shape with ‚å©mask‚å™equally length sides. (four)\\nProtoQA Prototypical Use simple words separated by commas to name something in\\nyour life that could cause you to lose weight. ( Eating less,\\nexercising more, stress.)' name='retrieve_documents' id='46111d62-d6ce-429b-b39a-ec5743e2afc3' tool_call_id='9eeb2952-cd32-4b5a-ad72-45cdffdde407'\n",
      "\n",
      "\n",
      "Update from node: rewrite_question\n",
      "----------------------------------------\n",
      "{'role': 'user', 'content': '<think>\\nOkay, the user asked \"Tell me about earth.\" I need to figure out what they really want. The word \"earth\" can mean different things. It could be the planet Earth, like the third rock from the sun, or it could refer to the soil, like the ground we walk on. Maybe they\\'re asking about the planet, but maybe they want information about the soil. \\n\\nWait, the original question is pretty vague. The user might not have specified which \"earth\" they\\'re interested in. So the improved question should clarify that. Let me think. If I ask \"Are you referring to the planet Earth or the soil (earth)?\", that would help narrow it down. But maybe there\\'s a better way. Alternatively, I could split it into two parts. For example, \"Could you clarify if you\\'re asking about the planet Earth or the soil (earth)?\" That way, the user can specify which one they want. \\n\\nAlternatively, maybe the user is asking about the Earth as a planet, but they might want more details. But since the original question is too broad, the improved version should ask for clarification. Let me check if there are other possible interpretations. Maybe \"earth\" could also refer to the element earth in the four elements, but that\\'s less likely. The main possibilities are planet Earth or soil. So the best approach is to ask the user to specify which one they mean. That way, the answer can be more accurate. So the improved question would be something like, \"Are you referring to the planet Earth or the soil (earth)?\" That should cover the main possibilities and ensure the user gets the information they need.\\n</think>\\n\\n**Improved Question:**  \\n\"Are you referring to the planet Earth or the soil (earth)?\"  \\n\\n**Reasoning:**  \\nThe original question is ambiguous because \"earth\" can refer to the planet, the ground/soil, or even the element. Clarifying the intent ensures the response addresses the user\\'s specific need, whether they want geological, astronomical, or environmental information.'}\n",
      "\n",
      "\n",
      "Update from node: generate_query_or_respond\n",
      "----------------------------------------\n",
      "content='<think>\\n</think>\\n\\n**Answer:**  \\nThe term \"earth\" can refer to two distinct concepts:  \\n\\n1. **Planet Earth**  \\n   - The third planet in our solar system, known for its diverse ecosystems, oceans, atmosphere, and unique conditions that support life.  \\n   - It is composed of layers including the crust (soil and rock), mantle, and core.  \\n\\n2. **Soil (Earth as ground)**  \\n   - The layer of mineral and organic material on the planet\\'s surface, formed by weathering of rocks and decomposition of organic matter.  \\n   - It is essential for plant growth and agriculture.  \\n\\nCould you clarify which concept you\\'re interested in? For example:  \\n- Are you asking about Earth as a planet (e.g., its geography, climate, or geology)?  \\n- Or are you referring to soil (e.g., its composition, role in ecosystems, or agricultural importance)?  \\n\\nThis will help provide a more accurate and detailed response! üåçüå±' additional_kwargs={} response_metadata={'model': 'qwen3', 'created_at': '2025-09-21T17:04:29.3980942Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1559867700, 'load_duration': 137182100, 'prompt_eval_count': 1282, 'prompt_eval_duration': 144704400, 'eval_count': 205, 'eval_duration': 1257754500, 'model_name': 'qwen3'} id='run--886be620-5284-4fc8-b38d-aacbf3c24ee8-0' usage_metadata={'input_tokens': 1282, 'output_tokens': 205, 'total_tokens': 1487}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Tell me about earth\"\n",
    "run_agentic_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1610de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-v1-projects (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
